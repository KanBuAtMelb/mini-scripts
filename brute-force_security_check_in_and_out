#!/usr/bin/env python
# coding: utf-8

import pandas as pd     
from datetime import datetime, timedelta, date
import datetime
import csv

from stat import S_ISREG, ST_CTIME, ST_MODE
import os, sys, time
from pathlib import Path


# # import files from "My Drive" from Google Drive
# from google.colab import drive
# drive.mount('/content/gdrive/', force_remount=True)

# # Upload from local driver
# from google.colab import files
# uploaded = files.upload()


# # Import raw data and pre-processing

# import files
# put r before your normal string. It converts a normal string to a raw string
path_K = r'C:\Daily data\Test\Daily\B1'
path_T = r'C:\Daily data\Test\Daily\B2'
path_Map = r'C:\Daily data\Test\Daily\Maps'
path_Log = r'C:\Daily data\Test\Daily\Logs'
path_Results = r'C:\Daily data\Test\Daily\Results'
# path_Achieved = r'C:\Daily data\Test\Daily\Archieved'

area_info_file = r'C:\Exemption areas\Area Info.xlsx'

# Row data column header names
date_time = 'Local System Time'
site_num = 'Store Number'
area_num = 'Area Number'
status = 'Security Status'
time = 'Time'
role = 'Role'
date = 'Date'
area_filter = 'Area Filter'
user_num = 'User Number'

brand = 'Brand'
site_name = 'Site Name'
user_name = 'User Name'
area_name = 'Area Name'

robot_areas_header = 'Robot Areas'
cleaner_areas_header = 'Cleaner Areas'
staff_entry_header = 'Staff Entry Area'

# Pre-processed data related parameters
accessed = 'Accessed'
secured = 'Secured'

# get all the file and sort by creation date (ascent order)
def sortedFiles(dirpath):
  # get all entries in the directory w/ stats
  entries = (os.path.join(dirpath, fn) for fn in os.listdir(dirpath))
  entries = ((os.stat(path), path) for path in entries)

  # leave only regular files, insert creation date
  entries = ((stat[ST_CTIME], path)
            for stat, path in entries if S_ISREG(stat[ST_MODE]))
  return sorted(entries)

# get all the file and sort by creation date (ascent order) in the file name
def sortedFilesByFName(dirpath):
  # get all entries in the directory with stats
  entries = [(fn, os.path.join(dirpath, fn)) for fn in os.listdir(dirpath) if fn.endswith('.csv')]
  entries = [(getDateFromFn(fn), path) for fn, path in entries]

  return sorted(entries)

# get date info from the file name
def getDateFromFn(fn):
  y,m,d = fn.split(' ')[1].split('-')[-3:]
  return datetime.date(int(y), int(m), int(d))


# dates when the secured areas are not found in the beginging of the logs. Use the first seven days
exempted_dates = pd.date_range(start="2022-10-31",end="2022-11-07").tolist()
exempted_dates = [i.date() for i in exempted_dates]

print(exempted_dates)


# ## Load and pre-processing B1 csv file

# get list of file (absolute path) sorted by creation date
entries_K = sortedFilesByFName(path_K)

file_count = 0
for ctime, path in entries_K:
  file_count +=1

print("Number of files: ", file_count)

# load the file with the latest date in the file name
ct, f_K = entries_K[file_count-1]
df_K = pd.read_csv(f_K,skipfooter=1,engine='python')

# drop off any row with blank cells, that removes all the footer line of each file
df_K.dropna(inplace = True)

# remove "System" line in "Panel" column
df_K =df_K[(df_K["Panel"]!="System")]


# ### Generate new mapping date

### work out the latest date of the PIPO files available
latest_file_path = entries_K[file_count-1][1]
new_map_date = os.path.basename(latest_file_path).split()[1].split('-',7)[7] 

print("map date: ",new_map_date)

new_map_filename = path_Map + "\mapping_" + new_map_date + ".csv"
print(new_map_filename)


# Create new log file

log_filename = path_Log + "\logging_"+ new_map_date + ".txt"
print(log_filename)

log_file= open(log_filename, "w")

# Pre-process B1 data 

# data frame for processing with both B1 and B2 data
df = pd.DataFrame()

# split columns
df_K[[site_num, brand,site_name]] = df_K["Panel"].str.split(' ',n=2,expand=True)
df_K[["Area and Action and User Number and User Name","Area Number and Area Name"]] = \
    df_K["Event description"].str.rsplit(', ', n=1, expand = True)
df_K[["Area",status,"by","User number and User Name"]] = \
    df_K["Area and Action and User Number and User Name"].str.split(' ',n=3,expand = True)
df_K[[user_num, user_name]] = df_K["User number and User Name"].str.split('(',n=1,expand = True)
df_K[[area_num,area_name]] = df_K["Area Number and Area Name"].str.split('(',n=1,expand = True)

# format Store Number
df_K[site_num] = pd.to_numeric(df_K[site_num], errors='coerce')
# drop off any blank values in 'Store Number'
df_K = df_K[df_K[site_num].notna()]
# convert store number to integer
df_K[site_num] = df_K.loc[:,site_num].astype('int')

# format Area Number
temp1 = df_K[area_num].str.strip()
temp2 = temp1.str.split(' ',expand=True)
df_K.loc[:,area_num] = temp2[1]
df_K[area_num] = pd.to_numeric(df_K[area_num], errors='coerce')
# drop NaN values
df_K = df_K[df_K[area_num].notna()]
# convert area number into integer
df_K[area_num] = df_K.loc[:,area_num].astype('int')

# format site number
df_K[site_num] = df_K.loc[:,site_num].astype('int')

# data frame for processing with both B1 and B2 data
df = pd.DataFrame()
df[date_time] = df_K["Local system time"]
df[site_num] = df_K.loc[:,site_num].astype(str)
df[status] = df_K[status].str.capitalize()
df[area_num] = 'Area '+df_K.loc[:,area_num].astype(str)
df[area_name] = df_K[area_name].str[:-1]
df[user_num] = df_K[user_num].str.strip()
df[user_name] = df_K[user_name].str[:-1]

# ## Load and pre-process B2 csv file

# get list of file (absolute path) sorted by creation date
entries_T = sortedFilesByFName(path_T)
file_count = 0
for ctime, path in entries_T:
  file_count +=1
ct, f_T = entries_T[file_count-1]
df_T = pd.read_csv(f_T)

# split columns
df_T[[site_num, brand,site_name]] = df_T["Panel"].str.split(' ',n=2,expand=True)
df_T[["Action","Area Number and Area Name","User Number and User Name"]] = df_T["Event description"].str.split(',', n=2, expand = True)
df_T[status] = df_T["Action"].str.split(' ').str[1]
df_T[["Area_Num_Lower",area_name]] = df_T["Area Number and Area Name"].str.split('(',expand = True)
df_T[area_num] = df_T["Area_Num_Lower"].str.capitalize()
df_T[["User_Num_Lower", user_name]] = df_T["User Number and User Name"].str.split('(',expand = True)
df_T["User_num_Capt"] = df_T["User_Num_Lower"].str.strip()
df_T[user_num] = df_T["User_num_Capt"].str.capitalize()

# format Store Number
df_T[site_num] = pd.to_numeric(df_T[site_num], errors='coerce')
df_T = df_T[df_T[site_num].notna()]
df_T[site_num] = df_T.loc[:,site_num].astype('int')

# format Area Number
temp1 = df_T[area_num].str.strip()
temp2 = temp1.str.split(' ',expand=True)
df_T[area_num] = temp2[1]
df_T[area_num] = pd.to_numeric(df_T[area_num], errors='coerce')
df_T = df_T[df_T[area_num].notna()]
df_T.loc[:,area_num].astype('int')

# format columns
df_T2 = pd.DataFrame()
df_T2[date_time] = df_T["Local system time"]
df_T2[site_num] = df_T[site_num].astype(str)
df_T2[status] = df_T[status].str.capitalize()
df_T2[area_num] = 'Area ' + df_T[area_num].astype(str)
df_T2[area_name] = df_T[area_name].str[:-1]
df_T2[user_num] = df_T[user_num].str.strip()
df_T2[user_name] = df_T[user_name].str[:-1]

# Merge B1 and B2 pre-processed data
df = df.merge(df_T2, how = 'outer')

# convert "Local System Time" column date type from String to Date Time
df[date_time] = pd.to_datetime(df[date_time], format="%d-%m-%Y %H:%M:%S",errors='coerce')

# split datetime into date and time
df[date] = df[date_time].dt.date
df[time] = df[date_time].dt.time #?? time precision needs double check

# Save the data frame to a csv file with the last B1 daily file date 
log_filename = path_Log + "\df_"+ new_map_date + ".csv"
df.to_csv(log_filename)

# Load "Area Info" 

# "Area Info.xlxs" loading
df_AI = pd.read_excel(area_info_file, index_col=None)
df_AI['Store Code'] = df_AI.loc[:, 'Store Code'].astype(str)


# Find the responding Cleaner Areas, TORY Areas or Staff Entry Area based on the given store number
def findAreaInfo(store_num, areas):
#   areas_str = df_AI.at[df_AI.loc[df_AI["Store Code"]==store_num].index[0],areas] # Unify "Store Code" and "Store Number"!!  
  try:
    areas_str = df_AI.at[df_AI.loc[df_AI["Store Code"]==store_num].index[0],areas]
    areas = set(areas_str.split(', ')) # The seperator is a comma and a space together!
  except:
    return set()
  return areas

# # Process data into PIPO by timeline

# site_list = df[site_num].unique()
site_list = df[site_num].unique()

date_list = df[date].unique()

print("There are {} sites, and {} dates, so the total iteration is {}".format(len(site_list), len(date_list), len(site_list)*len(date_list)))
# print(site_list)
print(date_list)

log_file.write("%s sites, and %s dates, so %s iterations.\n" % (len(site_list), len(date_list), len(site_list)*len(date_list)))

# global variables
staff_in = 'Staff In'
staff_out = 'Staff Out'
cleaner_in = 'Cleaner In'

min_gap =  timedelta(minutes = 15)


# ### Load the previous map
entries_M= sortedFiles(path_Map)
ct, path = entries_M[len(entries_M)-1]
print(ct)
print(path)
os.path.basename(path).split('_')[1].split('.')[0]


# load the previous map

# sort the files in Map folder to find the last mapping file
entries_M = sortedFiles(path_Map)
ct, old_map_path = entries_M[len(entries_M)-1]
old_map_date = os.path.basename(old_map_path).split('_')[1].split('.')[0]

map_item_qty = 4
key_accessed_areas = 0
key_cleaner_in = 1
key_staff_in = 2
key_staff_out = 3
key_store_status = 4

# validate the old map date is 1 day behind the new map date, otherwise stop the calculation
y,m,d = new_map_date.split('-')
date_new_map = datetime.date(int(y), int(m), int(d))

y,m,d = old_map_date.split('-')
date_old_map = datetime.date(int(y), int(m), int(d))

delta = date_new_map - date_old_map
if delta.days != 1:
    from sys import exit
    sys.exit("The latest map date does not match the new daily data log date by 1 day!")

# create site map dictionary
dict_map = {}
# load the csv file into the dictionary
with open(old_map_path, newline = '') as map_data:
  reader = csv.reader(map_data, delimiter=':')  
  for row in reader:
    site_number = row[0]
    dict_map[site_number] = []
    map_items = []
    if len(row)> 1:
      map_items = row[1][:-1].split(',')
      for i in range(map_item_qty):
        s = map_items[i]
        c = s[s.find('[')+1:s.find(']')]   
        dict_map[site_number].append(c.split('+')[:-1])
      store_status = map_items[map_item_qty]
      dict_map[site_number].append(store_status)

results_staff = []
results_cleaner = []
records_skipped = [] #log data to be saved for diagnosis
dict_map_new = {}

for i in range(len(site_list)):
  # find the store number
  this_site_num = site_list[i]

  # in case a store number is Nan
  if len(str(this_site_num)) < 4:
    print('the site {} is not valid'.format(this_site_num))
    continue
  else:
    print('Site #{}_______NEW SITE START: {}______________'.format(i, this_site_num))
    log_file.write('Site #%s_______NEW SITE START: %s______________ \n' % (i,this_site_num))

  # filter by the store number to get all the logs
  df_site = df[(df[site_num] == this_site_num)]  # the "df" need to be replaced with "dataset" in Power BI Python scripts

  # rest the row index from 0 
  df_site.reset_index(inplace=True)

  # total row number
  total_row = df_site.shape[0]

  # reload the accessed areas and other variables from the old map
#   print(dict_map[this_site_num])
  accessed_areas = set(dict_map[this_site_num][key_accessed_areas])
  
  treated_areas_for_cleaner = set(dict_map[this_site_num][key_cleaner_in])
  treated_areas_for_staff_in =set(dict_map[this_site_num][key_staff_in])
  treated_areas_for_staff_out= set(dict_map[this_site_num][key_staff_out])
  store_status = dict_map[this_site_num][key_store_status]

  # pre-fined robot areas, cleaner areas and staff entry:
  robot_areas = findAreaInfo(this_site_num, robot_areas_header)
  cleaner_areas = findAreaInfo(this_site_num, cleaner_areas_header)
  staff_entry = findAreaInfo(this_site_num, staff_entry_header)
 
  # iterate through each row of logs for a site
  for i in range(total_row):
    row = df_site.iloc[i]

    # remove Area 17-Area 99 
    areanum = int(row[area_num].split()[1])
    if areanum >= 17 and areanum <=99:
      print("Area number not valid, continue")
      continue

    # update last row info
    last_treated_areas_for_cleaner_len = len(treated_areas_for_cleaner)
    last_treated_areas_for_staff_in_len = len(treated_areas_for_staff_in)
    last_treated_areas_for_staff_out_len = len(treated_areas_for_staff_out)
    log_file.write('last staff_in, staff_Out, cleaner_in: {0}, {1}, {2} \n'.format(treated_areas_for_staff_in,treated_areas_for_staff_out,treated_areas_for_cleaner))

    # add/remove area based on security status to the accesed area list
    if row[status] == accessed: # if an area has been accessed, add it to the "accessed areas"
      accessed_areas.add(row[area_num])
    elif row[area_num] in accessed_areas: # if an area has been secured, remove it from the "accessed areas"
      accessed_areas.remove(row[area_num])
    elif row[date] in exempted_dates: # exemptions for first seven days
      print('Exempted date so skipped at Site {}: {} & {} & {} & {}'.format(this_site_num, row[date], row[time], row[area_num], row[status]) )
      continue
    else: # if an area is secured but not in the "accessed areas" due to potential data missing
      print('*******WARNING: date skipped at Site {}: {} & {} & {} & {}'.format(this_site_num, row[date], row[time], row[area_num], row[status]))
      records_skipped.append('date skipped at Site {}: {} & {} & {} & {}'.format(this_site_num, row[date], row[time], row[area_num], row[status]))
      continue
      log_file.write("# {0} Site {1}: {2} & {3} & {4} & {5} with accessed area list: {6} \n".format(i, this_site_num, row[date], row[time], row[area_num], row[status], accessed_areas))

    # treated areas for "Staff  In" timestamp capture - remove the cleaner areas, robot areas and staff entry areas
    treated_areas_for_staff_in = accessed_areas.copy()
    for area in robot_areas.union(cleaner_areas).union(staff_entry):
      if area in accessed_areas:
        treated_areas_for_staff_in.remove(area)
    

    # treated areas for "Staff Out" timestamp capture - remove robot areas and staff entry areas
    treated_areas_for_staff_out = accessed_areas.copy() 
    for area in robot_areas.union(staff_entry):
      if area in treated_areas_for_staff_out:
        treated_areas_for_staff_out.remove(area)

    # treated areas for "Cleaner In" timestamp capture - remove robot areas and staff entry areas
    treated_areas_for_cleaner = accessed_areas.copy() 
    for area in robot_areas.union(staff_entry):
      if area in treated_areas_for_cleaner:
        treated_areas_for_cleaner.remove(area)
    

    # define Staff-in
    ## a non-system user accesssed a FIRST area which are not the cleaner areas, robot areas or staff entry area
    if (row[user_num]!= 'User 0') & (row[status]== accessed) & (len(treated_areas_for_staff_in) == 1) & (last_treated_areas_for_staff_in_len == 0) & (store_status == secured):
      new_record = [this_site_num, staff_in, row[date_time]]
      last_result_index = len(results_staff)-1
      store_status = accessed
      if len(results_staff)==0: # if the list is empty, directly add the new record
        results_staff.append(new_record)
        print(' ++++++++ first record added as Staff In: ', new_record)
        log_file.write(' ++++++++ first record added as Staff In: %s \n' % new_record)
        continue
      elif (results_staff[last_result_index][0] == new_record[0]) & (results_staff[last_result_index][2] == new_record[2]) & (results_staff[last_result_index][1] == staff_out): # if the time stamp is the same as the previous and from the same site, then remove the previous
        to_be_removed = results_staff[len(results_staff)-1]
        results_staff.pop()
        print(' ++++++++ new record offsets the previous one. ', new_record, 'removed: ', to_be_removed)
        log_file.write('++++++++ new record offsets the previous one. {0}, removed: {1} \n'.format(new_record, to_be_removed))
        continue
      elif (results_staff[last_result_index][0] == new_record[0]) & ((new_record[2] - results_staff[last_result_index][2]) < min_gap) & ((new_record[2] > results_staff[last_result_index][2])): # if the new "Staff In" timestamp is within 15 min to the previous "Staff Out" timestamp, remove the previous "Staff Out" 
        if results_staff[last_result_index][1] == staff_out:
          to_be_removed = results_staff[len(results_staff)-1]
          results_staff.pop()
        print(' ++++++++ new record merges the previous one. ', new_record, 'removed: ', to_be_removed)
        log_file.write('++++++++ new record merges the previous one. {0}, removed: {1} \n'.format(new_record, to_be_removed))
        continue
      else:
        results_staff.append(new_record)
        print(' ++++++++ new record added as Staff In: ', new_record)
        log_file.write(' ++++++++ new record added as Staff In: %s \n' % new_record)
        continue
        

    # define Cleaner-in
    ## a non-system user accessed the FIRST cleaner area
    if (row[user_num]!= 'User 0') & (row[status] == accessed) & (row[area_num] in cleaner_areas) & (last_treated_areas_for_cleaner_len == 0):
      new_record = [this_site_num, cleaner_in, row[date_time]]
      if len(results_cleaner) == 0: # if the list is empty, directly add the new record
        results_cleaner.append(new_record)
        print('++++++++ new record added as Cleaner In: ', new_record)
        log_file.write('++++++++ new record added as Cleaner In: %s \n '% new_record)
        continue
      else: # if it is not the same site
          results_cleaner.append(new_record)
          print(' ++++++++ new record added as Cleaner In: ', new_record)
          log_file.write('++++++++ new record added as Cleaner In: %s \n '% new_record)
          continue


    # define Staff-out 
    ## a non-system user secured the LAST area regardless of the robot area and staff entry areas. Assume the store cannot be occupied normally with only the robot areas.
    if (row[status] == secured) & (len(treated_areas_for_staff_out) == 0) & (last_treated_areas_for_staff_out_len ==1)& (store_status == accessed):
      new_record = [this_site_num, staff_out, row[date_time]]
      last_result_index = len(results_staff)-1 # get the last piece of record index in the result list
      store_status = secured
      if len(results_staff) < 2: #?? WHY IT WAS "len(results_staff)<=2"??
        results_staff.append(new_record)
        print(' ++++++++ new record added as Staff Out: ', new_record)
        log_file.write(' ++++++++ new record added as Staff Out: %s \n' % new_record)
        continue
      elif (results_staff[last_result_index][0] == new_record[0]) & (results_staff[last_result_index][2] == new_record[2]) & (results_staff[last_result_index][1] == staff_in): # if the time stamp is the same as the previous "Staff In" event, and they are the same site, then offset both records 
        results_staff.pop()
        print(' ++++++++ new record offset the previous one. ', new_record)
        log_file.write(' ++++++++ new record offset the previous one. %s \n' % new_record)
        continue
      elif (results_staff[last_result_index][0] == new_record[0]) & ((new_record[2] - results_staff[last_result_index][2]) < min_gap) & ((new_record[2] > results_staff[last_result_index][2])): # if the new "Staff Out" timestamp is within 15 min to the previous "Staff In" timestamp, delete the previous "Staff In"
        if results_staff[last_result_index][1] == staff_in:
          to_be_removed = results_staff[len(results_staff)-1]
          results_staff.pop()
          print(' ++++++++ new record merged the previous one. ', new_record, 'removed: ', to_be_removed)
          log_file.write('++++++++ new record merged the previous one. {0}, removed: {1} \n'.format(new_record, to_be_removed))
        continue
      else:
        results_staff.append(new_record)
        print(' ++++++++ new record added as Staff Out: ', new_record)
        log_file.write(' ++++++++ new record added as Staff Out: %s \n' % new_record)
        continue

  # update the map of this site
  dict_map_new[this_site_num] = [accessed_areas,treated_areas_for_cleaner,treated_areas_for_staff_in,treated_areas_for_staff_out, store_status] 

# close log file
log_file.close()

results = results_staff.copy()
results.extend(results_cleaner)
df_results = pd.DataFrame(results, columns=['Store Code','Action','Date Time'])  

results_file = path_Results + "\\results_"+ new_map_date + ".csv"
df_results.to_csv(results_file)

dict_map_str = {}
for keys, values in dict_map_new.items():

  # transform the values to string
  # for example: "[Area 1+Area 2+Area 3],[Area 1+Area 2],[],[Secured]"
  l = "" # final long string
  s = "" # short string for each list item
  for i in range(len(values)):
    s = ""
    n = "" # temporary string with each list item
    if len(values[i]) == 0:
      s = "[]"
    elif isinstance(values[i],str):
      s = values[i]
    else:
      for item in values[i]:
        n = n + item +"+"
        # print("n is: ", n)
      s = "[" + n + "]"
    l = l + s + ","
  dict_map_str[keys] = l

print(new_map_filename)

with open(new_map_filename, mode = 'w') as f:
    for key in dict_map_str.keys():
        f.write("%s:%s\n"%(key,dict_map_str[key]))





# # Move B1 daily data to "Achieved" folder

# # Source folder
# src_folder = r'C:\Daily data\Test\Daily\B1'
# # Destination folder
# dst_folder = r'C:\Daily data\Test\Daily\Archieved'
# # Walk through the filesystem
# for dirpath, dirnames, filenames in os.walk(src_folder):
#     # Sort the dirnames to iterate over them in alphabetical order
#     dirnames.sort()
#     # Sort the filenames to iterate over them in alphabetical order
#     filenames.sort()
#     # Iterate over the files
#     for filename in filenames:
#         # Ignore hidden files
#         if filename.startswith('.'):
#             continue
#         # Only move one type of file
#         if not filename.endswith('.csv'):
#             continue
#         # Set the source path
#         src_path = Path(dirpath, filename)
#         # Only replace the first instance of the source folder's name
#         dst_dirpath = dirpath.replace(str(src_folder), dst_folder, 1)
#         dst_path = Path(dst_dirpath, filename)
#         # Check that the destination folder exists (create it if not)
#         os.makedirs(dst_dirpath, exist_ok=True)
#         # Move (which is actually a rename operation)
#         print(f'Moving "{src_path}" to "{dst_path}"')
#         os.rename(src_path, dst_path)





# # Move B1 daily data to "Achieved" folder

# # Source folder
# src_folder = r'C:\Daily data\Test\Daily\B2'
# # Destination folder
# dst_folder = r'C:\Daily data\Test\Daily\Archieved'
# # Walk through the filesystem
# for dirpath, dirnames, filenames in os.walk(src_folder):
#     # Sort the dirnames to iterate over them in alphabetical order
#     dirnames.sort()
#     # Sort the filenames to iterate over them in alphabetical order
#     filenames.sort()
#     # Iterate over the files
#     for filename in filenames:
#         # Ignore hidden files
#         if filename.startswith('.'):
#             continue
#         # Only move one type of file
#         if not filename.endswith('.csv'):
#             continue
#         # Set the source path
#         src_path = Path(dirpath, filename)
#         # Only replace the first instance of the source folder's name
#         dst_dirpath = dirpath.replace(str(src_folder), dst_folder, 1)
#         dst_path = Path(dst_dirpath, filename)
#         # Check that the destination folder exists (create it if not)
#         os.makedirs(dst_dirpath, exist_ok=True)
#         # Move (which is actually a rename operation)
#         print(f'Moving "{src_path}" to "{dst_path}"')
#         os.rename(src_path, dst_path)

